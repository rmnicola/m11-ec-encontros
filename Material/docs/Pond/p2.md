---
title: P2 - Visão computacional embarcada
sidebar_position: 2
sidebar_class_name: ponderada
---

# Visão computacional com hardware embarcado

**Atividade com prazo para 26/09/2024 às 23h59**

<img 
  src="https://64.media.tumblr.com/241f350955714d6ecde19f95119fc7e6/6bf90f6f6d80110a-a1/s640x960/5d6b018bdadfd7354caa0493a0c2ea8d6fc374e1.png"
  alt="Serpico"
  style={{ 
    display: 'block',
    marginLeft: 'auto',
    maxHeight: '50vh',
    marginRight: 'auto'
  }} 
/>
<br/>


Microcontroladores são dispositivos com mais restrições computacionais que
microprocessadores, mas mesmo assim eles são muito utilizados em diversos
cenários diferentes. Por quê isso?

O motivo é simples: microcontroladores, no geral, tem uma eficiência energética
muito maior e são dispositivos mais simples e mais **baratos**. Há, também, um
motivo um tanto escondido dos olhos mais leigos - a operação em *tempo real*.
Um microcontrolador não está sujeito a um sistema operacional com centenas de
milhares de processos concorrendo pelo mesmo recurso computacional. A
implicação disso é que qualquer coisa que você consiga rodar em um
microcontrolador sempre será como a prioridade máxima. *Só roda aquilo*,
portanto *só aquilo importa*. Existe um **determinismo harmônico** (bonito,
né?) por trás da execução das tarefas feitas por um microcontrolador.

O problema? Eu já disse lá em cima: eles tem muito mais restrições
computacionais. Fazer algo complexo rodar em um microcontrolador **é difícil**.
Vocês vão ter que fazer isso acontecer nessa ponderada.

## 1. Enunciado

Uma das coisas mais difíceis de fazer em um microcontrolador é fazer um modelo
de *deep learning* rodar em um dispositivo microcontrolado, pois isso
basicamente envolve demandar dele um volume de operações com *pontos
flutuantes* (o famoso float). Em um passado não tão distante,
microcontroladores nem mesmo tinham hardware que conseguia realizar nativamente
operações com números de pontos flutuantes. A boa notícia é que agora eles tem.
A má notícia é que ainda não é o suficiente para conseguir rodar frameworks de
*deep learning* complexos como o *Tensor Flow* ou o *Torch*. A notícia pior
ainda é que vocês vão ter que fazer um modelo de visão computacional rodar em
um microcontrolador mesmo assim.

**Por quê?**

Em sistemas cuja arquitetura prioriza latência em um volume de processamento
grande, deixar que os dados coletados sejam enviados a um servidor central para
processamento e, só então, retornar para o dispositivo na borda com o resultado
da operação é algo proibitivamente *lento* e que *sobrecarrega o sistema*. A
solução? Que tal processar já no dispositivo e enviar apenas o resultado da
operação para o sistema centralizado? Tá aí, é por isso.

**O que?**

Utilizando um microcontrolador e uma câmera, você deve criar um sistema capaz
de identificar uma face humana em tempo real. Para isso, você deve utilizar um
ESP32 para **capturar a imagem** e **enviar para um dispositivo remoto**. Esse
dispositivo remoto (e.g. o seu computador) deve processar a imagem e
identificar faces, exibindo tanto a imagem como os retângulos de detecção para
o usuário.

## 2. Padrão de qualidade

A divisão de notas para esta atividade é:

* Interface do microcontrolador com a câmera (precisa ser microcontrolador, o
  Raspberry Pi não se encaixa!!!) - até 7,0 pontos
* Processamento da imagem e exibição dos resultados - até 3,0 pontos

